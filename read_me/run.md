# PROSSIME RUN BISENET

| Run | Optimizer & lr                         | Scheduler                                                       | Loss                                                                                                                       | Tecniche Extra                                      | Note                                                                                                                                         |
| --- | -------------------------------------- | --------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| 1   | SGD (lr = 0.01, mom = 0.9, wd = 1e-4)  | Warmup 1100 iters 1e-6→0.01 + poly (power 0.9)                  | CE\_main + 1.0×(CE\_aux1 + CE\_aux2)                                                                                       | –                                                   | “Baseline” BiSeNet‐paper style. Dovrebbe posizionarsi intorno a 55–57% mIoU.                                                                 |
| 2   | SGD (lr = 0.015, mom = 0.9, wd = 1e-4) | Warmup 1100 iters 1e-6→0.015 + poly (power 0.9)                 | CE\_main + 1.0×(CE\_aux1 + CE\_aux2)                                                                                       | –                                                   | lr più alto per batch4; punta a raggiungere \~ 56–58% mIoU più rapidamente.                                                                  |
| 3   | SGD (lr = 0.025, mom = 0.9, wd = 1e-4) | Warmup 1100 iters 1e-6→0.025 + poly (power 0.9)                 | CE\_main + 1.0×(CE\_aux1 + CE\_aux2)                                                                                       | –                                                   | Valida configurazione “paper” standard, anche se batch piccolo: potenziale \~ 57–59% mIoU, se non esplode.                                   |
| 4   | SGD (lr = 0.01, mom = 0.9, wd = 1e-4)  | Warmup 1100 iters 1e-6→0.01 + poly (power 0.9)                  | CE\_main + 0.4×(CE\_aux1 + CE\_aux2)                                                                                       | –                                                   | Riduzione peso su auxiliary (α=0.4). Punta a concentrare più capacity sulla predizione finale: si attende un +0.5% di mIoU rispetto a Run 1. |
| 5   | SGD (lr = 0.01, mom = 0.9, wd = 1e-4)  | Warmup 1100 iters → 0.01 + poly (power 0.9)                     | – Epoch 1–35: CE\_main+1×(CE\_aux1+CE\_aux2)<br>– Ep 36–50: Lovász\_main + 1×(CE\_aux1+CE\_aux2)                           | –                                                   | Fine‐tuning con Lovász nelle ultime 15 epoche. Potrebbe aggiungere un +1% di mIoU alla curva CE.                                             |
| 6   | SGD (lr = 0.01, mom = 0.9, wd = 1e-4)  | Warmup 1100 iters → 0.01 + poly (power 0.9)                     | CE\_main + 1.0×(CE\_aux1 + CE\_aux2)                                                                                       | **Gradient Accumulation × 2** (batch effettivo = 8) | Batch “effettivo” 8 → BatchNorm più robusto. Spesso +1–2% di mIoU rispetto a Run 1, stesso lr.                                               |
| 7   | SGD (lr = 0.01, mom = 0.9, wd = 1e-4)  | Warmup 1100 iters → 0.01 + poly (power 0.9)                     | CE\_main + 1.0×(CE\_aux1 + CE\_aux2)                                                                                       | **Gradient Clipping (max\_norm=1.0)**               | Evita esplosioni di gradiente. Su batch 4 con architettura profonda, stabilizza il training → +0.3–0.5% di mIoU.                             |
| 8   | SGD (lr = 0.01, mom = 0.9, wd = 1e-4)  | **Warmup 2500 iters** (≈ 5 epoche) 1e-6→0.01 + poly (power 0.9) | CE\_main + 1.0×(CE\_aux1 + CE\_aux2)                                                                                       | –                                                   | Warmup esteso. Gestisce meglio lr alto in primissime epoche. Dovrebbe portare +0.5% di mIoU (meno instabilità).                              |
| 9   | **AdamW (lr = 0.001, wd = 1e-4)**      | Warmup 1100 iters da 1e-6→0.001 + poly (power 0.9)              | CE\_main + 1.0×(CE\_aux1 + CE\_aux2)                                                                                       | –                                                   | Ottimizzatore adattivo. A volte AdamW raggiunge plateau \~ 58% mIoU più velocemente di SGD.                                                  |
| 10  | SGD (lr = 0.01, mom = 0.9, wd = 1e-4)  | Warmup 1100 iters + poly (power 0.9)                            | **CE con class weights**:<br>`weight = total/(num_classes * counts[i])`<br>`total_loss = wCE_main + (wCE_aux1 + wCE_aux2)` | –                                                   | Assegna più importanza alle classi rare per “bilanciare” le frequenze di Cityscapes. Solitamente +0.5–1.0% di mIoU complessivo.              |
